# __Testing & QA Standards \(Expanded for AI Agent Workflow\)__

## __Importance in AI\-Assisted Development__

Maintaining rigorous testing and QA standards is paramount, particularly when leveraging AI agents for development\. AI can accelerate coding but may overlook edge cases, introduce subtle bugs, or lose context, leading to regressions or incomplete test coverage\. These standards serve as a critical quality gate and a way to ensure AI contributions are robust and reliable\. Strict adherence via automated checks and human review is non\-negotiable\.

## __Unit Testing__

- __Framework:__ Vitest \(preferred for Vite projects\) or Jest\.
- __Library:__ React Testing Library \(@testing\-library/react\) for component testing; standard assertion libraries for backend logic\.
- __Coverage:__ Aim for >80% statement coverage for critical business logic, utility functions, and complex components\. Coverage reports should be generated automatically in CI\.
- __Focus:__ Test individual functions, components, or classes in isolation\. Mock dependencies rigorously \(API calls, external services, database interactions\) using Vitest/Jest mocks, prisma\-mock, or MSW \(Mock Service Worker\)\.
- __AI Considerations:__
	- When prompting AI to generate unit tests, explicitly ask for tests covering:
		- __Happy paths:__ Standard, expected usage\.
		- __Failure paths:__ Error conditions, invalid inputs\.
		- __Edge cases:__ Null/undefined inputs, empty arrays, zero values, boundary conditions\.
	- __Review AI\-Generated Mocks:__ Carefully review any mocks generated by AI to ensure they accurately represent the dependency's behavior and cover necessary scenarios\.
- __Location:__ Place test files alongside the code they test \(e\.g\., MyComponent\.test\.tsx next to MyComponent\.tsx\)\.

// Example Unit Test \(Vitest \+ RTL\) \- Emphasizing Edge Cases  
import \{ render, screen \} from '@testing\-library/react';  
import \{ describe, it, expect, vi \} from 'vitest'; // Import vi for mocks  
import DataDisplay from '\./DataDisplay'; // Assuming component exists  
  
describe\('DataDisplay Component', \(\) => \{  
  it\('renders loading state correctly', \(\) => \{  
    render\(<DataDisplay isLoading=\{true\} data=\{null\} error=\{null\} />\);  
    expect\(screen\.getByText\(/loading/i\)\)\.toBeInTheDocument\(\);  
  \}\);  
  
  it\('renders error state correctly', \(\) => \{  
    const errorMessage = "Failed to fetch";  
    render\(<DataDisplay isLoading=\{false\} data=\{null\} error=\{errorMessage\} />\);  
    expect\(screen\.getByText\(new RegExp\(errorMessage, 'i'\)\)\)\.toBeInTheDocument\(\);  
  \}\);  
  
  it\('renders data correctly when provided', \(\) => \{  
    const testData = \[\{ id: 1, name: 'Item 1' \}\];  
    render\(<DataDisplay isLoading=\{false\} data=\{testData\} error=\{null\} />\);  
    expect\(screen\.getByRole\('listitem', \{ name: /item 1/i \}\)\)\.toBeInTheDocument\(\);  
  \}\);  
  
  // AI should be prompted to test edge cases like empty data  
  it\('renders empty state message when data is an empty array', \(\) => \{  
    render\(<DataDisplay isLoading=\{false\} data=\{\[\]\} error=\{null\} />\);  
    expect\(screen\.getByText\(/no items found/i\)\)\.toBeInTheDocument\(\); // Assuming an empty state message  
  \}\);  
  
   // AI should be prompted to test edge cases like null data  
  it\('renders empty state message when data is null', \(\) => \{  
    render\(<DataDisplay isLoading=\{false\} data=\{null\} error=\{null\} />\);  
    expect\(screen\.getByText\(/no items found/i\)\)\.toBeInTheDocument\(\);  
  \}\);  
\}\);  


## __Integration Testing__

- __Framework:__ Vitest/Jest, potentially with helpers like supertest for API integration tests\.
- __Focus:__ Test the interaction between multiple units/components or layers \(e\.g\., controller\-service\-repository\)\. Minimize mocking of the *direct collaborators* being tested, but mock external systems \(e\.g\., third\-party APIs, email services\)\.
- __Example Scenario:__ Testing a user registration flow might involve:
	1. Calling the registration API endpoint\.
	2. Verifying the controller calls the UserService\.
	3. Verifying the UserService interacts with Prisma to create a user\.
	4. Verifying the correct response is sent back\.  
\(Use a test database for these tests\)\.
- __AI Considerations:__ Pay close attention to testing the "seams" where AI\-generated code interacts with human\-written code or other AI\-generated modules\. Ensure contracts/interfaces are respected\.

## __End\-to\-End \(E2E\) Testing__

- __Framework:__ Cypress or Playwright\.
- __Focus:__ Simulate real user scenarios from start to finish in a dedicated test environment mimicking production\. Test critical user flows absolutely essential for business value\.
- __Strategy:__
	- Use data\-testid attributes for stable, resilient selectors\. Avoid relying on CSS classes or text content that might change frequently\.
	- Structure tests using patterns like Page Object Model \(POM\) for better organization and maintainability\.
	- Handle authentication via programmatic login commands or session restoration where possible to speed up tests\.
	- Implement retry logic for potentially flaky assertions \(use with caution\)\.
- __Test Environment:__ Requires a stable staging or dedicated E2E environment with seeded test data\.
- __CI/CD:__ Run E2E tests as part of the deployment pipeline \(e\.g\., after deploying to staging\)\. Consider running them on a schedule as well\.
- __AI Considerations:__ E2E tests are crucial for validating the overall output of AI\-generated features\. Ensure test descriptions are clear and descriptive, potentially usable as context for AI if debugging failures\.

// Example E2E Test \(Cypress\) \- Enhanced  
describe\('Critical User Flow: Creating and Viewing a Form', \(\) => \{  
  beforeEach\(\(\) => \{  
    // Programmatic login before each test in the suite  
    cy\.login\('testuser@example\.com', 'password123'\); // Custom command  
    cy\.visit\('/dashboard/forms'\);  
  \}\);  
  
  it\('allows a user to create a new form and see it in the list', \(\) => \{  
    const formTitle = \`My Test Form $\{Date\.now\(\)\}\`;  
  
    // Navigate to create form page  
    cy\.get\('\[data\-testid="create\-form\-button"\]'\)\.click\(\);  
    cy\.url\(\)\.should\('include', '/dashboard/forms/new'\);  
  
    // Fill out form details  
    cy\.get\('\[data\-testid="form\-title\-input"\]'\)\.type\(formTitle\);  
    cy\.get\('\[data\-testid="form\-description\-input"\]'\)\.type\('This is a test description\.'\);  
    // \.\.\. add interactions for adding fields \.\.\.  
  
    // Submit the form  
    cy\.get\('\[data\-testid="save\-form\-button"\]'\)\.click\(\);  
  
    // Assertions: Check for success notification and redirection  
    cy\.get\('\[data\-testid="success\-notification"\]'\)\.should\('contain\.text', 'Form created successfully'\);  
    cy\.url\(\)\.should\('include', '/dashboard/forms'\); // Should redirect back to list  
  
    // Assertions: Check if the new form appears in the list  
    cy\.get\('\[data\-testid="forms\-list"\]'\)\.should\('contain\.text', formTitle\);  
  \}\);  
  
  // Add tests for viewing, editing, deleting the form  
\}\);  


## __AI Agent Responsibilities in Testing \(New Section\)__

- __Test Generation:__ When generating feature code, AI agents __must__ also generate corresponding unit tests covering happy paths, failures, and edge cases\. Prompts should explicitly request this\.
- __Test Updates:__ When refactoring or modifying existing code, AI agents __must__ update related tests to ensure they still pass and accurately reflect the changes\. Prompts for refactoring should include this requirement\.
- __Test Execution & Interpretation:__ AI agents should be capable of running test suites via CLI commands \(pnpm test\) and interpreting the output to identify failures\.
- __Debugging Failures:__ If tests fail after AI modifications, the agent should attempt to debug the issue, using test failure output as context\.

Example Prompt Snippet for AI:

"\.\.\.Implement the calculateTotal function based on the requirements\. Additionally, generate comprehensive unit tests using Vitest, covering scenarios with zero items, multiple items, items with zero price, and invalid input types\."

## __Strict Enforcement__

- __CI/CD Gates:__ Linting, formatting, unit tests \(with coverage checks\), and potentially integration/E2E tests __must__ pass in the CI pipeline before code can be merged into develop or main\. No exceptions\.
- __Pre\-commit Hooks:__ Use Husky \+ lint\-staged to run linters, formatters, and potentially related unit tests *before* a commit is even created locally\. This catches errors early\.

## __Test Data Management__

- __Avoid Production Data:__ Never use real user or sensitive production data in any test environment\.
- __Factories/Fixtures:__ Use libraries like factory\.ts or custom fixture generators to create consistent and reusable test data\.
- __Seeding:__ Use database seeding scripts \(e\.g\., Prisma seed\) to populate test databases for integration and E2E tests\.
- __Anonymization:__ If sampling production data is absolutely necessary \(use with extreme caution\), implement robust anonymization scripts\.

## __Human Review__

- __AI\-Generated Tests:__ All tests generated by AI agents require human review to assess quality, coverage, and relevance, just like AI\-generated feature code\.
- __Test Plans:__ For complex features, a human should define the high\-level test plan and critical E2E scenarios\.

## __Accessibility & Performance Testing__

- Refer to 16\_Accessibility\_Compliance\.md and 17\_Performance\_Optimization\_Standards\.md\.
- Integrate automated checks \(Axe, Lighthouse\) into the CI pipeline\. Manual checks remain essential\.

## __Code Quality Checks__

- Enforced via Linters \(ESLint\), Formatters \(Prettier\), and CI/CD pipeline checks\. \(No changes needed here, just reinforcement\)\.

